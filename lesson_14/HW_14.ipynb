{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90913e72",
   "metadata": {},
   "source": [
    "–ó–∞–¥–∞–Ω–∏–µ\n",
    "  \n",
    "–≤–∑—è—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑  \n",
    "https://www.kaggle.com/datasets/mrapplexz/bashim-quotes  \n",
    "–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–∏—Ö —Ü–∏—Ç–∞—Ç  \n",
    "  \n",
    "–≤–∑—è—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑  \n",
    "https://github.com/natasha/corus    \n",
    "load_lenta2  \n",
    "–Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è —Å–∞–º —Ç–µ–∫—Å—Ç –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫  \n",
    "–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å T5/ –∏–ª–∏ GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–µ–π  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2aafc",
   "metadata": {},
   "source": [
    "# –∑–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bb222",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/mrapplexz/bashim-quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db5ea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bankholdup/rugpt3_song_writer\"\n",
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a00b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "092f16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfc83120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers.trainer import logger as noisy_logger\n",
    "noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d073b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = pd.read_json('dataset.jsonl', lines=True).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "whole-sally",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81497, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30d4b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = df_rec.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0a4ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clear_text(text):\n",
    "    clr_text = re.sub(r\"<.*?>\", \" \", text).lower()\n",
    "    clr_text = summary = re.sub(r\"\\s\", \" \", clr_text)\n",
    "    return clr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51408568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>451487</th>\n",
       "      <td>2018-07-27 07:12:00+00:00</td>\n",
       "      <td>2262.0</td>\n",
       "      <td>–ö–∫–∫–∫—Ä–æ—Ç–æ–≤: –æ–ø–µ—á–∞—Ç–∫–∞ \"–∑–æ–≤ –ø–µ—Ä–µ–¥–∫–æ–≤\" —Ç–æ—á–Ω–µ–µ –æ—Ç—Ä–∞...</td>\n",
       "      <td>–∫–∫–∫–∫—Ä–æ—Ç–æ–≤: –æ–ø–µ—á–∞—Ç–∫–∞ \"–∑–æ–≤ –ø–µ—Ä–µ–¥–∫–æ–≤\" —Ç–æ—á–Ω–µ–µ –æ—Ç—Ä–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398613</th>\n",
       "      <td>2008-08-22 10:54:00+00:00</td>\n",
       "      <td>38421.0</td>\n",
       "      <td>–ê–Ω–Ω–∞**–ó–ê–ö–û–ù–ß–ò–õ–ê –®–ö–û–õ–£!!!\"–í–∏–∫—Ç–æ—Ä–æ–≤–∞ –≤—Å—Ç—É–ø–∏–ª–∞ –≤ ...</td>\n",
       "      <td>–∞–Ω–Ω–∞**–∑–∞–∫–æ–Ω—á–∏–ª–∞ —à–∫–æ–ª—É!!!\"–≤–∏–∫—Ç–æ—Ä–æ–≤–∞ –≤—Å—Ç—É–ø–∏–ª–∞ –≤ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456044</th>\n",
       "      <td>2019-05-29 07:13:00+00:00</td>\n",
       "      <td>2362.0</td>\n",
       "      <td>stupidchemist: –ù–∞ –º–æ—ë –º–µ—Å—Ç–æ \"–ø—Ä–æ—â–µ —Ö–∏–º–∏–∫–æ–≤ —Ä–∞–∑...</td>\n",
       "      <td>stupidchemist: –Ω–∞ –º–æ—ë –º–µ—Å—Ç–æ \"–ø—Ä–æ—â–µ —Ö–∏–º–∏–∫–æ–≤ —Ä–∞–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431520</th>\n",
       "      <td>2014-12-16 08:12:00+00:00</td>\n",
       "      <td>14257.0</td>\n",
       "      <td>XXX: –ú–Ω–µ –Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤ 90-–µ, —è –±—ã —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ...</td>\n",
       "      <td>xxx: –º–Ω–µ –Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤ 90-–µ, —è –±—ã —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397561</th>\n",
       "      <td>2008-06-30 13:54:00+00:00</td>\n",
       "      <td>28256.0</td>\n",
       "      <td>–í–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–π –≤–æ–∑–≥–ª–∞—Å –Ω–∞—à–µ–≥–æ –ø—Ä–µ–ø–æ–¥–∞ –ø–æ –ª–æ–≥–∏–∫–µ:...</td>\n",
       "      <td>–≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–π –≤–æ–∑–≥–ª–∞—Å –Ω–∞—à–µ–≥–æ –ø—Ä–µ–ø–æ–¥–∞ –ø–æ –ª–æ–≥–∏–∫–µ:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            date   rating  \\\n",
       "id                                          \n",
       "451487 2018-07-27 07:12:00+00:00   2262.0   \n",
       "398613 2008-08-22 10:54:00+00:00  38421.0   \n",
       "456044 2019-05-29 07:13:00+00:00   2362.0   \n",
       "431520 2014-12-16 08:12:00+00:00  14257.0   \n",
       "397561 2008-06-30 13:54:00+00:00  28256.0   \n",
       "\n",
       "                                                     text  \\\n",
       "id                                                          \n",
       "451487  –ö–∫–∫–∫—Ä–æ—Ç–æ–≤: –æ–ø–µ—á–∞—Ç–∫–∞ \"–∑–æ–≤ –ø–µ—Ä–µ–¥–∫–æ–≤\" —Ç–æ—á–Ω–µ–µ –æ—Ç—Ä–∞...   \n",
       "398613  –ê–Ω–Ω–∞**–ó–ê–ö–û–ù–ß–ò–õ–ê –®–ö–û–õ–£!!!\"–í–∏–∫—Ç–æ—Ä–æ–≤–∞ –≤—Å—Ç—É–ø–∏–ª–∞ –≤ ...   \n",
       "456044  stupidchemist: –ù–∞ –º–æ—ë –º–µ—Å—Ç–æ \"–ø—Ä–æ—â–µ —Ö–∏–º–∏–∫–æ–≤ —Ä–∞–∑...   \n",
       "431520  XXX: –ú–Ω–µ –Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤ 90-–µ, —è –±—ã —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ...   \n",
       "397561  –í–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–π –≤–æ–∑–≥–ª–∞—Å –Ω–∞—à–µ–≥–æ –ø—Ä–µ–ø–æ–¥–∞ –ø–æ –ª–æ–≥–∏–∫–µ:...   \n",
       "\n",
       "                                               clear_text  \n",
       "id                                                         \n",
       "451487  –∫–∫–∫–∫—Ä–æ—Ç–æ–≤: –æ–ø–µ—á–∞—Ç–∫–∞ \"–∑–æ–≤ –ø–µ—Ä–µ–¥–∫–æ–≤\" —Ç–æ—á–Ω–µ–µ –æ—Ç—Ä–∞...  \n",
       "398613  –∞–Ω–Ω–∞**–∑–∞–∫–æ–Ω—á–∏–ª–∞ —à–∫–æ–ª—É!!!\"–≤–∏–∫—Ç–æ—Ä–æ–≤–∞ –≤—Å—Ç—É–ø–∏–ª–∞ –≤ ...  \n",
       "456044  stupidchemist: –Ω–∞ –º–æ—ë –º–µ—Å—Ç–æ \"–ø—Ä–æ—â–µ —Ö–∏–º–∏–∫–æ–≤ —Ä–∞–∑...  \n",
       "431520  xxx: –º–Ω–µ –Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤ 90-–µ, —è –±—ã —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ...  \n",
       "397561  –≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–π –≤–æ–∑–≥–ª–∞—Å –Ω–∞—à–µ–≥–æ –ø—Ä–µ–ø–æ–¥–∞ –ø–æ –ª–æ–≥–∏–∫–µ:...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec[\"clear_text\"] = df_rec[\"text\"].apply(lambda x: clear_text(x))\n",
    "df_rec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5333d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_rec.loc[:, 'clear_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fca456b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "451487    –∫–∫–∫–∫—Ä–æ—Ç–æ–≤: –æ–ø–µ—á–∞—Ç–∫–∞ \"–∑–æ–≤ –ø–µ—Ä–µ–¥–∫–æ–≤\" —Ç–æ—á–Ω–µ–µ –æ—Ç—Ä–∞...\n",
       "398613    –∞–Ω–Ω–∞**–∑–∞–∫–æ–Ω—á–∏–ª–∞ —à–∫–æ–ª—É!!!\"–≤–∏–∫—Ç–æ—Ä–æ–≤–∞ –≤—Å—Ç—É–ø–∏–ª–∞ –≤ ...\n",
       "456044    stupidchemist: –Ω–∞ –º–æ—ë –º–µ—Å—Ç–æ \"–ø—Ä–æ—â–µ —Ö–∏–º–∏–∫–æ–≤ —Ä–∞–∑...\n",
       "431520    xxx: –º–Ω–µ –Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤ 90-–µ, —è –±—ã —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ...\n",
       "397561    –≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–π –≤–æ–∑–≥–ª–∞—Å –Ω–∞—à–µ–≥–æ –ø—Ä–µ–ø–æ–¥–∞ –ø–æ –ª–æ–≥–∏–∫–µ:...\n",
       "                                ...                        \n",
       "417334    na-ta: —Å—Ä–æ—á–Ω–æ! –∑–∞—Ç–æ–ø–∏–ª–∏ —Å–æ—Å–µ–¥–µ–π! —Ö–µ–ª–ø! –ø–æ—Å–ª–µ 8...\n",
       "430027    xxx: –Ω–∞—à—ë–ª –≤ –ª–µ–Ω—Ç–µ –∫—Ä–∏–∫ –¥—É—à–∏ –∫–∞–∫–æ–≥–æ-—Ç–æ —à–∫–æ–ª—å–Ω–∏...\n",
       "442396    —Ö—Ö—Ö: –∫–∞–ª–∏—Ñ–æ—Ä–Ω–∏—è —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ä–µ—Ñ–µ—Ä–µ–Ω–¥—É–º –ø–æ –æ—Ç—Å–æ–µ...\n",
       "399826    xxx —Ö–æ—á—É 201—Ä–æ–∑—É  xxx –Ω–µ –≤—ã–π–¥—É –∑–∞ —Ç–µ–±—è,–ø–æ–∫–∞ —Ç—ã...\n",
       "414686    —Ö—Ö—Ö: —Å–∏–¥–∏–º –≤ –æ–¥–Ω–æ–π –∫–æ–º–Ω–∞—Ç–µ —Å –Ω–∞—á–∞–ª—å–Ω–∏–∫–æ–º –æ—Ç–¥–µ–ª...\n",
       "Name: clear_text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "207d5ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_text_files(data_json, dest_path):\n",
    "#     f = open(dest_path, 'w')\n",
    "    with open(dest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        data = ''\n",
    "        for texts in data_json:\n",
    "            summary = str(texts).strip()\n",
    "    #         summary = re.sub(r\"<.*?>\", \" \", summary)\n",
    "    #         summary = re.sub(r\"\\s\", \" \", summary)\n",
    "            data += summary + \"  \"\n",
    "    #     with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(html)\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07da444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1668f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be7c73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 8500\n",
      "Test dataset length: 1500\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset length: \"+ str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a2782d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "415125    —çon: –¥–æ–ø—É—Å—Ç–∏–º, —è —Ö–æ—á—É —Ç–µ–ª–µ–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∏–∑ —Ç–æ—á...\n",
       "392895    —Ö—Ö—Ö: —è —Å–µ–≥–æ–¥–Ω—è –≤—ã—á–∏—Å–ª–∏–ª, –∑–∞ —Å–∫–æ–ª—å–∫–æ –ø—ã–ª–µ—Å–æ—Å –≤—ã...\n",
       "431520    xxx: –º–Ω–µ –Ω—Ä–∞–≤–∏–ª–æ—Å—å –≤ 90-–µ, —è –±—ã —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ...\n",
       "226142    nansee: –≤–æ—Ç –∫—Ä–∞—à—É—Å—å –∏ –¥—É–º–∞—é: –∞ –≤–µ–¥—å –≤ —Ñ–æ—Ç–æ—à–æ–ø–µ...\n",
       "399450    –∫–∞—Ç–µ—Ä–∏–Ω–∞ >>  –º–∏—Ö, –∞ —á—Ç–æ —Ç–∞–∫–æ–µ —Ä–∞–ø–∏–¥—à–∞—Ä–∞?  tera...\n",
       "Name: clear_text, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a66d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "#sberbank-ai/rugpt3large_based_on_gpt2\n",
    "#sberbank-ai/rugpt3medium_based_on_gpt2\n",
    "#sberbank-ai/rugpt3small_based_on_gpt2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8bbae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voron\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\data\\datasets\\language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130074b",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8d63a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51805732edbc416d841fc4ea20b6085b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "869a7ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voron\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\cuda\\__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    \"phrase\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e09a1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d15e57f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voron\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2190' max='2190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2190/2190 7:02:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.091000</td>\n",
       "      <td>3.931326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.965500</td>\n",
       "      <td>3.919243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2190, training_loss=4.033529481931364, metrics={'train_runtime': 25376.7375, 'train_samples_per_second': 0.345, 'train_steps_per_second': 0.086, 'total_flos': 572098904064000.0, 'train_loss': 4.033529481931364, 'epoch': 2.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e5018",
   "metadata": {},
   "source": [
    "# generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c22a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prefix):\n",
    "    tokens = tokenizer(prefix, return_tensors='pt')\n",
    "    size = tokens['input_ids'].shape[1]\n",
    "\n",
    "    output = model.generate(\n",
    "        **tokens, \n",
    "        #end_token=end_token_id,\n",
    "        do_sample=False,\n",
    "        max_length=size+50, \n",
    "        early_stopping=True,\n",
    "        length_penalty=2.0,\n",
    "        repetition_penalty=8., \n",
    "        temperature=0.5,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=5\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0])\n",
    "    result = decoded[len(prefix):]\n",
    "    return prefix + result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60aee52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–Ω—É —Ç—ã —Å–æ–±–∏—Ä–∞–µ—à—å—Å—è –∏–¥—Ç–∏?  xxx: —É –Ω–∞—Å –≤ –≥–æ—Ä–æ–¥–µ –µ—Å—Ç—å –æ–¥–∏–Ω –º–∞–≥–∞–∑–∏–Ω, –≥–¥–µ –º–æ–∂–Ω–æ –∫—É–ø–∏—Ç—å –≤—Å–µ —á—Ç–æ –¥—É—à–µ —É–≥–æ–¥–Ω–æ. yyy: –∞ —è –≤–æ—Ç –Ω–µ –º–æ–≥—É —Å–µ–±–µ –ø–æ–∑–≤–æ–ª–∏—Ç—å —Ç–∞–∫—É—é —Ä–æ—Å–∫–æ—à—å - –ø–æ–∫—É–ø–∞—Ç—å –≤—Å—è–∫—É—é —Ñ–∏–≥–Ω—é –Ω–∞ —Ä–∞–∑–≤–µ—Å –∏ —Ö–æ–¥–∏—Ç—å —Å –Ω–µ–π –ø–æ –º–∞–≥–∞–∑–∏–Ω—É...\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"–Ω—É —Ç—ã —Å–æ–±–∏—Ä–∞–µ—à—å—Å—è –∏–¥—Ç–∏?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e888394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–∑–∞–≤–æ–¥–∏, –ø–æ–µ—Ö–∞–ª–∏.  xxx: –∞ —É –Ω–∞—Å –≤ –≥–æ—Ä–æ–¥–µ –µ—Å—Ç—å —Ç–∞–∫–æ–π –º–∞–≥–∞–∑–∏–Ω—á–∏–∫ - \"—Å—É–≤–µ–Ω–∏—Ä—ã\". —Ç–∞–º –º–æ–∂–Ω–æ –∫—É–ø–∏—Ç—å —á—Ç–æ-–Ω–∏–±—É–¥—å –Ω–∞ –ø–∞–º—è—Ç—å –æ –¥–µ—Ç—Å—Ç–≤–µ –∏ —é–Ω–æ—Å—Ç–∏... yyy: –Ω—É —Ç–∞–∫ –≤–æ—Ç —è —Ç–µ–±–µ —Å–µ–π—á–∞—Å —Ä–∞—Å—Å–∫–∞–∂—É –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ —Ç–æ, –∫–∞–∫\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"–∑–∞–≤–æ–¥–∏, –ø–æ–µ—Ö–∞–ª–∏\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77d61101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–∑–∞—Ö–≤–∞—Ç–∏ –≤ –º–∞–≥–∞–∑–∏–Ω–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –∫ —á–∞—é.  xxx: —É –º–µ–Ω—è –µ—Å—Ç—å –∑–Ω–∞–∫–æ–º—ã–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –∑–∞–≤–æ–¥–µ –ø–æ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–µ –Ω–µ—Ñ—Ç–∏ –∏ –≥–∞–∑–∞ (–≤—Ö–æ–¥–∏—Ç –≤ —Ö–æ–ª–¥–∏–Ω–≥ \"–Ω–µ—Ñ—Ç–µ–≥–∞–∑–æ–≤–∞—è –∫–æ–º–ø–∞–Ω–∏—è\") - –æ–Ω –æ—á–µ–Ω—å –ª—é–±–∏—Ç —Å–≤–æ—é —Ä–∞–±–æ—Ç—É! yyy: –∞ —è –≤–æ—Ç –Ω–µ –ª—é–±–ª—é\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"–∑–∞—Ö–≤–∞—Ç–∏ –≤ –º–∞–≥–∞–∑–∏–Ω–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –∫ —á–∞—é\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-bailey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
